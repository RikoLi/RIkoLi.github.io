<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>机器学习笔记：支持向量机SVM基本原理 | 逢田文明博物馆</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="什么是支持向量机（SVM）最原始的SVM是一个线性二分类器，它的根本目的是寻找一个可以将训练样本划分开的高维空间内的超平面。很明显，这种超平面有很多，但是哪一个是最好的呢？">
<meta name="keywords" content="机器学习,支持向量机">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习笔记：支持向量机SVM基本原理">
<meta property="og:url" content="http://zjuriko.ml/2018/09/10/机器学习笔记：支持向量机SVM基本原理/index.html">
<meta property="og:site_name" content="逢田文明博物馆">
<meta property="og:description" content="什么是支持向量机（SVM）最原始的SVM是一个线性二分类器，它的根本目的是寻找一个可以将训练样本划分开的高维空间内的超平面。很明显，这种超平面有很多，但是哪一个是最好的呢？">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2018-09-24T13:15:21.709Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习笔记：支持向量机SVM基本原理">
<meta name="twitter:description" content="什么是支持向量机（SVM）最原始的SVM是一个线性二分类器，它的根本目的是寻找一个可以将训练样本划分开的高维空间内的超平面。很明显，这种超平面有很多，但是哪一个是最好的呢？">
  
    <link rel="alternate" href="/atom.xml" title="逢田文明博物馆" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">逢田文明博物馆</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">一个说闲话的地方</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://zjuriko.ml"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-机器学习笔记：支持向量机SVM基本原理" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/10/机器学习笔记：支持向量机SVM基本原理/" class="article-date">
  <time datetime="2018-09-10T15:52:59.000Z" itemprop="datePublished">2018-09-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      机器学习笔记：支持向量机SVM基本原理
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="什么是支持向量机（SVM）"><a href="#什么是支持向量机（SVM）" class="headerlink" title="什么是支持向量机（SVM）"></a>什么是支持向量机（SVM）</h1><p>最原始的SVM是一个线性二分类器，它的根本目的是寻找一个可以将训练样本划分开的高维空间内的超平面。很明显，这种超平面有很多，但是哪一个是最好的呢？</p>
<a id="more"></a>
<p>评价作为边界的超平面优秀与否的判据是：这个超平面是否能产生鲁棒性最强的分类结果，泛化能力最强。</p>
<p>因此，在众多能划分开训练样本的超平面中，我们认为，距离各样本尽可能“远”<a href="这里“远”指的是，将被分类的各类训练样本集合作为一个整体，这些集合彼此之间的“距离”尽可能“远&quot;。我必须承认，这种定义非常不严谨，但是我还是认为这样解释比较好理解。从后文可知，“使尽可能远”似乎可以等价为“最大化间隔$\gamma$”，这样解释似乎更加严谨。">^1</a>的那个超平面是最好的分界超平面，我们要找到这个尽可能远的距离的表达式并优化它。这个超平面可由它所划分的两类训练样本中距离该平面<strong>最近</strong>的几个样本所确定，因此这几个样本得名<strong>支持向量</strong>（Support Vector），它们“支持”了分类超平面。</p>
<h1 id="如何确定SVM的分类超平面"><a href="#如何确定SVM的分类超平面" class="headerlink" title="如何确定SVM的分类超平面"></a>如何确定SVM的分类超平面</h1><h2 id="决策边界：最佳的分类超平面"><a href="#决策边界：最佳的分类超平面" class="headerlink" title="决策边界：最佳的分类超平面"></a>决策边界：最佳的分类超平面</h2><p>怎么找到最佳的分类超平面呢？首先，我们列出要得到的超平面的几条性质，然后我们使用这些性质构建出超平面满足的约束关系，从而得到超平面。</p>
<p>分类超平面的性质：</p>
<ul>
<li>它是一个超平面，可以用线性组合表示</li>
<li>它距离它所划分的两类训练样本中支持向量的距离一样远</li>
<li>它距离它所划分的两类训练样本中的支持向量尽可能远</li>
</ul>
<p>于是我们可以得出关于超平面的约束如下，首先要满足是个平面，一般我们称其为“决策边界”(Decision Boundary)：<br>$$DB: \bold{w}^T\bold{x} + b = 0$$</p>
<p>上式中，$\bold{w}$为权重向量（SVM中我们要优化的目标），$\bold{x}$为训练样本的输入向量，b为偏差值。满足上式描述的输入向量$\bold{x}$全部存在于以$\bold{w}$为唯一参数决定的超平面上。</p>
<p>我们要衡量训练样本到超平面的距离，使用点到超平面距离公式描述任意一个训练样本（我们使用编号i表示这个样本）到超平面的距离：<br>$$d_i = \frac{|\bold{w}^T\bold{x}_i+b|}{||\bold{w}||}$$</p>
<p>注意上式中$\bold{x}_i$表示第i个训练样本的输入向量。</p>
<h2 id="间隔和最大间隔假设"><a href="#间隔和最大间隔假设" class="headerlink" title="间隔和最大间隔假设"></a>间隔和最大间隔假设</h2><p>要继续推导SVM的原理，还要明确两个概念：间隔和最大间隔假设。</p>
<p>上文提到，决策边界要离支持向量尽可能远，且两类的支持向量距离决策边界距离相等，这说明被分类的两类支持向量分别存在于两个平行于决策边界的超平面上，它们距决策边界的距离相同。于是，决策边界和两个由支持向量构成的超平面划分出了一个不存在训练样本的区域，这个区域的大小称为“间隔”（Margin），我们用$\gamma$表示它。</p>
<p>由于决策边界和两个支持向量超平面距离相同，确定一个支持向量超平面距离决策边界的距离乘以二即可得到$\gamma$。可是，根据不同的训练样本情况，一个支持向量超平面和决策边界的距离似乎可以是任意实数c，这为$\gamma$的确定引入了新的未知量c。</p>
<p>不过，可以证明[^2]无论c取多少，都可以通过缩放变换将其变为1，这就为确定$\gamma$提供了计算上的方便。</p>
<p>[^2]: 如果没记错，AndrewNg在他的机器学习讲义中证明了，此处不赘述。（<del>实际上是我忘了2333</del>）</p>
<p>c取1，也就是说我们确定了两个由支持向量确定的超平面，它们刻画了训练样本的正确分类：<br>$$\begin{cases}<br>\bold{w}^T\bold{x}<em>+ + b \ge 1,\space{} y</em>+ = 1 \<br>\bold{w}^T\bold{x}<em>- + b \le 1,\space{} y</em>- = -1<br>\end{cases}$$</p>
<p>上式中，训练样本分为正类和负类，正确分类时，正类结果为1，负类结果为-1，支持向量所在平面即为取等号是对应的两个超平面。上式即为“最大间隔假设”。</p>
<p>还记得我们最开始的目的吗？让决策边界离训练样本尽可能远，现在我们有了$\gamma$来衡量“远近”，因此我们目的就是使$\gamma$尽可能大。上文提到了点到超平面距离，我们将其用在支持向量上，于是我们得到间隔的表达式：<br>$$\gamma = \frac{2}{||\bold{w}||}$$</p>
<p>可喜可贺，至此我们已经得到了要优化的表达式了！</p>
<h1 id="如何优化-gamma"><a href="#如何优化-gamma" class="headerlink" title="如何优化$\gamma$"></a>如何优化$\gamma$</h1><p>SVM的构建至此已经基本完成，剩下的就是数学上的处理了。</p>
<p>整理一下我们推导出的结论，我们目前要做的是：<br>$$\begin{gathered}\max_{\bold{w}, b} \frac{2}{||\bold{w}||} \<br>s.t.\space{} y_i(\bold{w}^T\bold{x}_i+b) \ge 1, \space{} i = 1,2,…n<br>\end{gathered}$$</p>
<p>其中，第二个式子是最大间隔假设的两个式子的合在一起写的形式。下标i表示第i个样本，我们假设一共n个样本，因此一共有n条限制条件，其含义为全部n个样本全部被正确分类。</p>
<p>很明显，我们需要尽量使第一个式子的分母小，为了优化方便，我们考虑优化下面等价的式子，<strong>SVM的本质就是这两个式子：</strong><br>$$\begin{gathered}\min_{\bold{w}, b} \frac{1}{2} ||\bold{w}||^2 \<br>s.t.\space{}y_i(\bold{w}^T\bold{x}_i+b) \ge 1, \space{} i = 1,2,…n<br>\end{gathered}$$</p>
<p>怎么优化呢？乍一看，这是个凸二次规划问题，是有固定解法的，不过对于SVM还有更高效的解法。对于所有约束条件优化问题，我们可以使用SMO算法进行优化<a href="使用SMO进行优化可以参考[https://www.cnblogs.com/pinard/p/6111471.html](https://www.cnblogs.com/pinard/p/6111471.html)">^3</a>，这点让我们当做这篇文章的题外话吧。</p>
<h2 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h2><p>上述待优化的问题可以转换成对偶问题求解，使原函数尽可能小的对偶问题是使得对偶的函数（拉格朗日函数）尽可能大。构建拉格朗日函数：<br>$$L(\bold{w}, b, \bold{\alpha}) = \frac{1}{2} ||\bold{w}||^2 + \sum_{i=1}^{n}\alpha_i(1-y_i(\bold{w}^T\bold{x}_i+b))$$</p>
<p>将$L$对$\bold{w}$和$b$求偏导数，并令偏导数等于零，对于每一个分量：<br>$$\begin{aligned}<br>\frac{\partial{L}}{\partial{w_j}} &amp;= w_j - \alpha_iy_ix_i^{(j)} = 0 \<br>\frac{\partial{L}}{\partial{b}} &amp;= \alpha_iy_i = 0<br>\end{aligned}$$</p>
<p>写成向量表示：<br>$$\begin{aligned}<br>\bold{w} &amp;= \sum_{i=1}^n\alpha_iy_i\bold{x}<em>i \<br>0 &amp;= \sum</em>{i=1}^n\alpha_iy_i<br>\end{aligned}$$</p>
<p>将上述两式代回$L$，消去$\bold{w}$和$b$，得到：<br>$$L = \sum_{i=1}^n\alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\bold{x}_i^T\bold{x}_j, \space{} \alpha_i \ge 0$$</p>
<p>接下来我们要做的就是求使得$L$最大化的$\bold{\alpha}$，进而可得$\bold{w}$和$b$。</p>
<h1 id="再进一步"><a href="#再进一步" class="headerlink" title="再进一步"></a>再进一步</h1><p>我们已经推导得出基本的线性可分二分类硬间隔SVM的数学表达了，接下来我们来看看怎么让这个模型更加贴近实用，我们还需要补充一些内容。</p>
<h2 id="核函数（Kernel-Function）"><a href="#核函数（Kernel-Function）" class="headerlink" title="核函数（Kernel Function）"></a>核函数（Kernel Function）</h2><p>核函数的引入帮助我们将SVM从只支持线性可分拓展到支持线性不可分。</p>
<p>对非线性可分问题，我们将原特征空间中线性不可分的样本点通过某类映射$\phi(\cdot)$映射到高维特征空间中，在这个特征空间中它们是线性可分的。之前的推导中需要计算特征向量的内积$\bold{x}_i^T\bold{x}_j$，经过映射后对应高维特征向量也需要计算内积$\phi(\bold{x}_i)^T\phi(\bold{x}_j)$，可是因为向量维数增多，直接计算高维向量内积很困难，于是我们构想一个函数$k(\cdot, \cdot)$使其<strong>能在原始特征空间中使用原始特征向量直接计算高维特征向量内积的结果</strong>，这种函数称作<strong>核函数</strong>。</p>
<h2 id="软间隔"><a href="#软间隔" class="headerlink" title="软间隔"></a>软间隔</h2><p>核函数似乎已经解决了线性不可分问题，实际上在真实环境下，找到一个完美将特征向量分开的核函数是极其困难的，因此我们要允许SVM在一定数量的数据上“出错”，这些出错的数据点要被允许出现在SVM的间隔内，这种方案称为<strong>软间隔</strong>。</p>
<p>“出错”我们表示为不满足SVM的约束条件：<br>$$<br>y_i(\bold{w}^T\bold{x}_i+b) \ge 1<br>$$</p>
<p>于是我们将优化目标修改为：<br>$$<br>\min_{\bold{w},b}\frac{1}{2}||\bold{w}||^2+C\sum_{i=1}^nloss(y_i(\bold{w}^T\bold{x}_i+b)-1)<br>$$</p>
<p>我们使用$loss(\cdot)$决定“错误”判断的样本点，根据这个函数的不同选择，我们可以得到不同的SVM。使用对率损失函数$l_{log}=log(1+e^{-z})$可以得到多分类SVM的雏形。如果只考虑二分类，更多使用hinge损失函数$l_{hinge}=max(0, 1-z)$。</p>
<p>使用hinge损失函数的情况中，我们一般将损失函数的每一项记作<strong>松弛变量</strong>$\xi_i$，因此优化目标写为：<br>$$<br>\min_{\bold{w},b}\frac{1}{2}||\bold{w}||^2+C\sum_{i=1}^n\xi_i<br>$$</p>
<p>$$<br>\xi_i = max(0, 1-y_i(\bold{w}^T\bold{x}_i+b))<br>$$</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://zjuriko.ml/2018/09/10/机器学习笔记：支持向量机SVM基本原理/" data-id="cjtwdr5v0000lhkdu8jk41xrl" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/支持向量机/">支持向量机</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/10/14/关于矩阵几何意义的理解/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          关于矩阵几何意义的理解
        
      </div>
    </a>
  
  
    <a href="/2018/08/11/Python-Tkinter库参考手册/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Python Tkinter库参考手册</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/GAN/">GAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/JavaScript/">JavaScript</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/人工智能/">人工智能</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/其他/">其他</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/支持向量机/">支持向量机</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习/">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/矩阵/">矩阵</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/随笔/">随笔</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/GAN/" style="font-size: 10px;">GAN</a> <a href="/tags/JavaScript/" style="font-size: 10px;">JavaScript</a> <a href="/tags/Python/" style="font-size: 16.67px;">Python</a> <a href="/tags/人工智能/" style="font-size: 10px;">人工智能</a> <a href="/tags/其他/" style="font-size: 10px;">其他</a> <a href="/tags/支持向量机/" style="font-size: 10px;">支持向量机</a> <a href="/tags/机器学习/" style="font-size: 13.33px;">机器学习</a> <a href="/tags/深度学习/" style="font-size: 20px;">深度学习</a> <a href="/tags/矩阵/" style="font-size: 10px;">矩阵</a> <a href="/tags/随笔/" style="font-size: 16.67px;">随笔</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/03/31/枯了/">枯了</a>
          </li>
        
          <li>
            <a href="/2019/03/26/GraphSAGE：Node Embedding初探&复现的一些想法/">GraphSAGE：Node Embedding初探&amp;复现的一些想法</a>
          </li>
        
          <li>
            <a href="/2019/03/24/2019.3.23水水亚巡场记/">2019.3.23水水亚巡场记</a>
          </li>
        
          <li>
            <a href="/2019/03/19/一些回忆的事/">一些回忆的事</a>
          </li>
        
          <li>
            <a href="/2019/03/09/AI整理/">AI整理</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Riko Li<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>